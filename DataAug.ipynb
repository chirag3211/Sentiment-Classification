{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      ".vector_cache/glove.6B.zip: 862MB [02:52, 5.00MB/s]                               \n",
      "100%|█████████▉| 399999/400000 [00:22<00:00, 17777.19it/s]\n",
      "100%|██████████| 149985/149985 [00:05<00:00, 25922.18it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torchtext.vocab as vocab\n",
    "from scipy.spatial.distance import cosine\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert_model.to(device)\n",
    "\n",
    "glove = vocab.GloVe(name='6B', dim=300)\n",
    "\n",
    "mask_token_id = tokenizer.mask_token_id\n",
    "substitution_percentage = 0.1\n",
    "\n",
    "glove_vectors = vocab.GloVe(name='6B', dim=300)\n",
    "\n",
    "def get_most_similar_glove_embedding(glove, word):\n",
    "    try:\n",
    "        word_vector = glove.vectors[glove.stoi[word]]\n",
    "        similarities = 1 - cosine(glove.vectors, word_vector)\n",
    "        most_similar_index = np.argmax(similarities)\n",
    "        return glove.itos[most_similar_index]\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "# Function to substitute subwords with GloVe embeddings\n",
    "def substitute_subwords_with_glove(sentence, tokens, positions, glove, tokenizer):\n",
    "    for position in positions:\n",
    "        token = tokens[position]\n",
    "        tokenized_word = tokenizer.tokenize(token)\n",
    "\n",
    "        if len(tokenized_word) > 1:  # Word tokenized into subwords\n",
    "            # Replace subwords with most similar GloVe embedding\n",
    "            closest_word = get_most_similar_glove_embedding(glove, token)\n",
    "            if closest_word:\n",
    "                tokens[position] = closest_word\n",
    "\n",
    "# Function to augment data\n",
    "def augment_data(sentence, percentage=0.1):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    num_words_to_replace = int(len(tokens) * percentage)\n",
    "\n",
    "    positions_to_replace = np.random.choice(len(tokens), size=num_words_to_replace, replace=False)\n",
    "    \n",
    "    substitute_subwords_with_glove(sentence, tokens, positions_to_replace, glove_vectors, tokenizer)\n",
    "\n",
    "    augmented_sentence = tokenizer.convert_tokens_to_string(tokens)\n",
    "    return augmented_sentence\n",
    "\n",
    "# Load the data from the text file\n",
    "# Assuming the file is tab-separated and doesn't have a header\n",
    "data = pd.read_csv('train_150k.txt', sep='\\t', header=None, names=['label', 'text'])\n",
    "\n",
    "# Convert the DataFrame into a list of tuples\n",
    "data_tuples = [tuple(x) for x in data.to_records(index=False)]\n",
    "\n",
    "def augment_row(row):\n",
    "    # Unpack the row\n",
    "    label, text = row\n",
    "    \n",
    "    # Augment the text\n",
    "    augmented_text = augment_data(text)\n",
    "    \n",
    "    # Return a new row with the same label and augmented text\n",
    "    return (label, augmented_text)\n",
    "\n",
    "# Create a pool of workers\n",
    "with Pool() as p:\n",
    "    # Apply the data augmentation to each row in the DataFrame in parallel\n",
    "    augmented_data = list(tqdm(p.imap(augment_row, data_tuples), total=len(data)))\n",
    "\n",
    "# Convert the augmented data to a DataFrame\n",
    "augmented_data = pd.DataFrame(augmented_data, columns=['label', 'text'])\n",
    "\n",
    "# Save the augmented data back to a text file\n",
    "augmented_data.to_csv('augmented_data.txt', sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sample",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
